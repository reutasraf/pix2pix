{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH8WTL_OOE9a"
      },
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('drive')\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsfkL_5dzDs8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhi1Tvj-PDcl"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        sourcePath = os.path.join(root, 'face') +\"/\"+mode+ \"/*.*\"\n",
        "        print(sourcePath)\n",
        "        self.sourceFiles = sorted(glob.glob(sourcePath))\n",
        "        destPath = os.path.join(root, 'comics') +\"/\"+mode+ \"/*.*\"\n",
        "        self.destFiles = sorted(glob.glob(destPath))\n",
        "        if mode == \"train\":\n",
        "            sourcePath = os.path.join(root, 'face') + \"/test\" + \"/*.*\"\n",
        "            destPath = os.path.join(root, 'comics') + \"/test\" + \"/*.*\"\n",
        "    \n",
        "            self.sourceFiles.extend(sorted(glob.glob(sourcePath)))\n",
        "            self.destFiles.extend(sorted(glob.glob(destPath)))\n",
        "\n",
        "        print(1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img_B = Image.open(self.sourceFiles[index % len(self.sourceFiles)])\n",
        "        img_A = Image.open(self.destFiles[index % len(self.destFiles)])\n",
        "\n",
        "\n",
        "        if np.random.random() < 0.5:\n",
        "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "\n",
        "        img_A = self.transform(img_A)\n",
        "        img_B = self.transform(img_B)\n",
        "\n",
        "        return {\"A\": img_A, \"B\": img_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sourceFiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ZrOdR_PGeT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "##############################\n",
        "#           U-NET\n",
        "##############################\n",
        "\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "        super(UNetDown, self).__init__()\n",
        "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_size))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.0):\n",
        "        super(UNetUp, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # U-Net generator with skip connections from encoder to decoder\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "\n",
        "        return self.final(u7)\n",
        "\n",
        "\n",
        "##############################\n",
        "#        Discriminator\n",
        "##############################\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate image and condition image by channels to produce input\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        return self.model(img_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqPByxYRPJPA"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# from models import *\n",
        "# from datasets2pic import *\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "def main():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n",
        "  # parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
        "  parser.add_argument(\"--n_epochs\", type=int, default=15, help=\"number of epochs of training\")\n",
        "\n",
        "  parser.add_argument(\"--dataset_name\", type=str, default=\"pix2pix\", help=\"name of the dataset\")\n",
        "  # parser.add_argument(\"--dataset_name\", type=str, default=\"facades\", help=\"name of the dataset\")\n",
        "\n",
        "  parser.add_argument(\"--batch_size\", type=int, default=10, help=\"size of the batches\")\n",
        "  parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "  parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "  parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "  parser.add_argument(\"--decay_epoch\", type=int, default=100, help=\"epoch from which to start lr decay\")\n",
        "  parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
        "  parser.add_argument(\"--img_height\", type=int, default=256, help=\"size of image height\")\n",
        "  parser.add_argument(\"--img_width\", type=int, default=256, help=\"size of image width\")\n",
        "  parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "  parser.add_argument(\n",
        "      \"--sample_interval\", type=int, default=500, help=\"interval between sampling of images from generators\"\n",
        "  )\n",
        "  parser.add_argument(\"--checkpoint_interval\", type=int, default=2, help=\"interval between model checkpoints\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  opt = parser.parse_args()\n",
        "  print(opt)\n",
        "\n",
        "  os.makedirs(\"/content/drive/MyDrive/data/images1/%s\" % opt.dataset_name, exist_ok=True)\n",
        "  os.makedirs(\"/content/drive/MyDrive/data/saved_models1/%s\" % opt.dataset_name, exist_ok=True)\n",
        "\n",
        "  cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "  # Loss functions\n",
        "  criterion_GAN = torch.nn.MSELoss()\n",
        "  criterion_pixelwise = torch.nn.L1Loss()\n",
        "\n",
        "  # Loss weight of L1 pixel-wise loss between translated image and real image\n",
        "  lambda_pixel = 100\n",
        "\n",
        "  # Calculate output of image discriminator (PatchGAN)\n",
        "  patch = (1, opt.img_height // 2 ** 4, opt.img_width // 2 ** 4)\n",
        "\n",
        "  # Initialize generator and discriminator\n",
        "  generator = GeneratorUNet()\n",
        "  discriminator = Discriminator()\n",
        "\n",
        "  if cuda:\n",
        "      generator = generator.cuda()\n",
        "      discriminator = discriminator.cuda()\n",
        "      criterion_GAN.cuda()\n",
        "      criterion_pixelwise.cuda()\n",
        "\n",
        "  opt.epoch = 12;\n",
        "  if opt.epoch != 0:\n",
        "      # Load pretrained models\n",
        "      generator.load_state_dict(torch.load(\"/content/drive/MyDrive/data/saved_models1/%s/generator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
        "      discriminator.load_state_dict(torch.load(\"/content/drive/MyDrive/data/saved_models1/%s/discriminator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
        "  else:\n",
        "      # Initialize weights\n",
        "      generator.apply(weights_init_normal)\n",
        "      discriminator.apply(weights_init_normal)\n",
        "\n",
        "  # Optimizers\n",
        "  optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "  optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "  # Configure dataloaders\n",
        "  transforms_ = [\n",
        "      transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "  ]\n",
        "  print(opt.dataset_name)\n",
        "  dataloader = DataLoader(\n",
        "    ImageDataset(\"/content/drive/MyDrive/data/%s\" % opt.dataset_name, transforms_=transforms_),\n",
        "      batch_size=opt.batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=2,\n",
        "  )\n",
        "\n",
        "  val_dataloader = DataLoader(\n",
        "      ImageDataset(\"/content/drive/MyDrive/data/%s\" % opt.dataset_name, transforms_=transforms_, mode=\"val\"),\n",
        "      batch_size=10,\n",
        "      shuffle=True,\n",
        "      num_workers=2,\n",
        "  )\n",
        "\n",
        "  # Tensor type\n",
        "  Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "  def sample_images(batches_done):\n",
        "      \"\"\"Saves a generated sample from the validation set\"\"\"\n",
        "      imgs = next(iter(val_dataloader))\n",
        "      real_A = Variable(imgs[\"B\"].type(Tensor))\n",
        "      real_B = Variable(imgs[\"A\"].type(Tensor))\n",
        "      fake_B = generator(real_A)\n",
        "      img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
        "      save_image(img_sample, \"/content/drive/MyDrive/data/images1/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)\n",
        "\n",
        "\n",
        "  # ----------\n",
        "  #  Training\n",
        "  # ----------\n",
        "\n",
        "  prev_time = time.time()\n",
        "\n",
        "  for epoch in range(opt.epoch, opt.n_epochs):\n",
        "      for i, batch in enumerate(dataloader):\n",
        "\n",
        "          # Model inputs\n",
        "          real_A = Variable(batch[\"B\"].type(Tensor))\n",
        "          real_B = Variable(batch[\"A\"].type(Tensor))\n",
        "\n",
        "          # Adversarial ground truths\n",
        "          valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
        "          fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
        "\n",
        "          # ------------------\n",
        "          #  Train Generators\n",
        "          # ------------------\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # GAN loss\n",
        "          fake_B = generator(real_A)\n",
        "          pred_fake = discriminator(fake_B, real_A)\n",
        "          loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "          # Pixel-wise loss\n",
        "          loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
        "\n",
        "          # Total loss\n",
        "          loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
        "\n",
        "          loss_G.backward()\n",
        "\n",
        "          optimizer_G.step()\n",
        "\n",
        "          # ---------------------\n",
        "          #  Train Discriminator\n",
        "          # ---------------------\n",
        "\n",
        "          optimizer_D.zero_grad()\n",
        "\n",
        "          # Real loss\n",
        "          pred_real = discriminator(real_B, real_A)\n",
        "          loss_real = criterion_GAN(pred_real, valid)\n",
        "\n",
        "          # Fake loss\n",
        "          pred_fake = discriminator(fake_B.detach(), real_A)\n",
        "          loss_fake = criterion_GAN(pred_fake, fake)\n",
        "\n",
        "          # Total loss\n",
        "          loss_D = 0.5 * (loss_real + loss_fake)\n",
        "\n",
        "          loss_D.backward()\n",
        "          optimizer_D.step()\n",
        "\n",
        "          # --------------\n",
        "          #  Log Progress\n",
        "          # --------------\n",
        "\n",
        "          # Determine approximate time left\n",
        "          batches_done = epoch * len(dataloader) + i\n",
        "          batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
        "          time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "          prev_time = time.time()\n",
        "\n",
        "          import sys\n",
        "          # Print log\n",
        "          sys.stdout.write(\n",
        "              \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
        "              % (\n",
        "                  epoch,\n",
        "                  opt.n_epochs,\n",
        "                  i,\n",
        "                  len(dataloader),\n",
        "                  loss_D.item(),\n",
        "                  loss_G.item(),\n",
        "                  loss_pixel.item(),\n",
        "                  loss_GAN.item(),\n",
        "                  time_left,\n",
        "              )\n",
        "          )\n",
        "\n",
        "          # If at sample interval save image\n",
        "          if batches_done % opt.sample_interval == 0:\n",
        "              sample_images(batches_done)\n",
        "\n",
        "      if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
        "          # Save model checkpoints\n",
        "          torch.save(generator.state_dict(), \"/content/drive/MyDrive/data/saved_models1/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\n",
        "          torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/data/saved_models1/%s/discriminator_%d.pth\" % (opt.dataset_name, epoch))\n",
        "\n",
        "  torch.save(generator.state_dict(), \"/content/drive/MyDrive/data/saved_models1/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\n",
        "  torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/data/saved_models1/%s/discriminator_%d.pth\" % (opt.dataset_name, epoch))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  \n",
        "  sys.argv = ['']\n",
        "  \n",
        "  del sys\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}